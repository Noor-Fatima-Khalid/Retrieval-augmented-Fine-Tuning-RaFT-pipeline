{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnpMj9bnT9Wv"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers faiss-cpu nltk tqdm pandas openpyxl xlrd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# Load your sentence transformer model\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
      ],
      "metadata": {
        "id": "HBtdP21eVrNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "KB_FILE_PATH = \"/content/OOP Knowledge Base.txt\"\n",
        "\n",
        "with open(KB_FILE_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    KB_TEXT = f.read()\n",
        "\n",
        "print(\"KB loaded. Characters:\", len(KB_TEXT))\n",
        "# print(KB_TEXT[:2000])\n",
        "\n",
        "uploaded_dataset = files.upload()\n",
        "\n",
        "DATASET_PATH = list(uploaded_dataset.keys())[0]\n",
        "\n",
        "df = pd.read_excel(DATASET_PATH)\n",
        "\n",
        "print(\"Dataset loaded\")\n",
        "\n",
        "# Normalize column names\n",
        "df.columns = df.columns.str.strip().str.lower()\n",
        "print(\"Normalized columns:\", df.columns.tolist())\n",
        "\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "print(\"Total rows:\", len(df))"
      ],
      "metadata": {
        "id": "MU6VagWnWN_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_kb_by_concept_clean(text):\n",
        "    concepts = []\n",
        "    current = []\n",
        "\n",
        "    # Split text into concepts by lines starting with \"Name:\"\n",
        "    for line in text.split(\"\\n\"):\n",
        "        if line.strip().startswith(\"Name:\"):\n",
        "            if current:\n",
        "                concepts.append(\"\\n\".join(current))\n",
        "                current = []\n",
        "        current.append(line)\n",
        "    if current:\n",
        "        concepts.append(\"\\n\".join(current))\n",
        "\n",
        "    # Section headings\n",
        "    headings = [\n",
        "        \"Definition\",\"Key Properties\",\"Related Concepts\",\"How to Implement\",\n",
        "        \"Variants / Language Differences\",\"Common Patterns\",\"Why It Exists\",\n",
        "        \"Trade-offs\",\"Best Practices\",\"Code Snippet\",\"Real-World Analogy\",\n",
        "        \"Common Mistakes\",\"Consequences\",\"How to Avoid\",\n",
        "        \"Questions the concept can answer\",\"Cross-concept connections\"\n",
        "    ]\n",
        "    heading_pattern = r\"(?=(\" + \"|\".join([re.escape(h) for h in headings]) + r\"):)\"\n",
        "\n",
        "    chunks = []\n",
        "    metadata = []\n",
        "\n",
        "    for concept in concepts:\n",
        "        # Extract concept name\n",
        "        name_match = re.search(r\"Name:\\s*(.*)\", concept)\n",
        "        concept_name = name_match.group(1).strip() if name_match else \"Unknown\"\n",
        "\n",
        "        # Split by headings\n",
        "        sections = re.split(heading_pattern, concept)\n",
        "        buffer = \"\"\n",
        "        for s in sections:\n",
        "            if s.strip() in headings:\n",
        "                if buffer.strip():\n",
        "                    chunks.append(buffer.strip())\n",
        "                    metadata.append(concept_name)\n",
        "                buffer = s\n",
        "            else:\n",
        "                buffer += \"\\n\" + s\n",
        "\n",
        "        if buffer.strip():\n",
        "            chunks.append(buffer.strip())\n",
        "            metadata.append(concept_name)\n",
        "\n",
        "    # Filter tiny chunks (<5 words)\n",
        "    final_chunks = []\n",
        "    final_metadata = []\n",
        "    for c, m in zip(chunks, metadata):\n",
        "        if len(c.split()) >= 5:\n",
        "            final_chunks.append(c)\n",
        "            final_metadata.append(m)\n",
        "\n",
        "    return final_chunks, final_metadata"
      ],
      "metadata": {
        "id": "qCcwwR2aYeQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def clean_chunk_text(chunk):\n",
        "    \"\"\"Cleans chunk by removing metadata lines and trimming whitespace.\"\"\"\n",
        "    lines = chunk.split(\"\\n\")\n",
        "    cleaned = []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        if line.startswith((\"Name:\", \"Category:\", \"Difficulty:\", \"Tags:\")):\n",
        "            continue\n",
        "        line = line.rstrip(\":\").strip()\n",
        "        cleaned.append(line)\n",
        "    return \"\\n\".join(cleaned)\n",
        "\n",
        "def merge_chunks_unique(chunks):\n",
        "    \"\"\"Merge chunks removing duplicate lines/headings.\"\"\"\n",
        "    merged = []\n",
        "    seen_lines = set()\n",
        "    for chunk in chunks:\n",
        "        for line in chunk.split(\"\\n\"):\n",
        "            line = line.strip()\n",
        "            if line and line not in seen_lines:\n",
        "                merged.append(line)\n",
        "                seen_lines.add(line)\n",
        "    return \"\\n\".join(merged)\n",
        "\n",
        "def retrieve_context_for_question(question, k=8):\n",
        "    \"\"\"\n",
        "    Retrieve top-k KB chunks for a question, grouped by concept,\n",
        "    and merge chunks uniquely per concept.\n",
        "\n",
        "    Returns a list of strings, each in the format:\n",
        "    \"Concept: <concept_name>\\n<merged_text>\"\n",
        "    \"\"\"\n",
        "    # Embed the question\n",
        "    q_emb = embedder.encode([question], convert_to_numpy=True)\n",
        "\n",
        "    # Search the index for top-k nearest neighbors\n",
        "    distances, indices = index.search(q_emb, k)\n",
        "\n",
        "    # Group retrieved chunks by concept\n",
        "    grouped = defaultdict(list)\n",
        "    for idx in indices[0]:\n",
        "        concept = kb_metadata[idx]        # get the concept\n",
        "        chunk = kb_chunks[idx]            # get the chunk\n",
        "        grouped[concept].append(chunk)\n",
        "\n",
        "    # Merge chunks per concept and format output\n",
        "    merged_contexts = [\n",
        "        f\"Concept: {concept}\\n{merge_chunks_unique(chunks)}\"\n",
        "        for concept, chunks in grouped.items()\n",
        "    ]\n",
        "\n",
        "    return merged_contexts\n"
      ],
      "metadata": {
        "id": "7mUn06DQhp33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === FINAL PIPELINE CELL (FIXED) ===\n",
        "from tqdm import tqdm\n",
        "import faiss\n",
        "\n",
        "# --------------------------\n",
        "# 1️⃣ Chunk the KB\n",
        "# --------------------------\n",
        "kb_chunks, kb_metadata = chunk_kb_by_concept_clean(KB_TEXT)\n",
        "print(f\"Total chunks after concept split: {len(kb_chunks)}\")\n",
        "\n",
        "# --------------------------\n",
        "# 2️⃣ Clean all chunks\n",
        "# --------------------------\n",
        "kb_chunks = [clean_chunk_text(c) for c in kb_chunks]\n",
        "\n",
        "# --------------------------\n",
        "# 3️⃣ Encode chunks and build FAISS index\n",
        "# --------------------------\n",
        "chunk_embeddings = embedder.encode(kb_chunks, convert_to_numpy=True)\n",
        "dim = chunk_embeddings.shape[1]\n",
        "\n",
        "index = faiss.IndexFlatL2(dim)\n",
        "index.add(chunk_embeddings)\n",
        "print(f\"FAISS index built with {index.ntotal} chunks.\")\n",
        "\n",
        "# --------------------------\n",
        "# 4️⃣ Generate detailed context for each question in dataset\n",
        "# --------------------------\n",
        "K_TOP = 8  # number of top chunks to retrieve per question\n",
        "detailed_contexts = []\n",
        "\n",
        "for question in tqdm(df[\"questions\"], desc=\"Generating detailed context\"):\n",
        "    # Retrieve concept-level chunks (list of strings)\n",
        "    concept_chunks = retrieve_context_for_question(question, k=K_TOP)\n",
        "\n",
        "    # Merge all concepts into a single multi-line string\n",
        "    merged_context = \"\\n\\n\".join(concept_chunks)\n",
        "\n",
        "    # Add follow-up prompts for the AI interviewer\n",
        "    follow_up_prompts = (\n",
        "        \"\\n\\nFollow-Up Prompts for AI Interviewer:\\n\"\n",
        "        \"1. Explain why this concept is important.\\n\"\n",
        "        \"2. Provide a code example demonstrating it.\\n\"\n",
        "        \"3. Describe common mistakes developers make.\\n\"\n",
        "        \"4. Discuss trade-offs and best practices.\\n\"\n",
        "    )\n",
        "\n",
        "    # Append final string to list\n",
        "    detailed_contexts.append(merged_context + follow_up_prompts)\n",
        "\n",
        "# Assign to DataFrame\n",
        "df[\"detailed-context\"] = detailed_contexts\n",
        "print(\"✅ Detailed context column added to DataFrame\")"
      ],
      "metadata": {
        "id": "1eRN495N_eYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "OUTPUT_PATH = \"OOP_dataset.xlsx\"\n",
        "\n",
        "df.to_excel(OUTPUT_PATH, index=False)\n",
        "\n",
        "print(f\"✅ Excel file saved as: {OUTPUT_PATH}\")\n",
        "\n",
        "files.download(OUTPUT_PATH)\n"
      ],
      "metadata": {
        "id": "fMcTVwVVnBOR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}